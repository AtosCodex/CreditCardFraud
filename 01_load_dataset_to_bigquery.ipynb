{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation\n",
    "0. Install python required libraries\n",
    "1. Create data directory\n",
    "2. Download credit card fraud detection dataset to local filesystem\n",
    "3. Create google cloud storage bucket for project\n",
    "4. Create google bigquery dataset\n",
    "5. Upload credit card fraud detection dataset to bigquery dataset with auto detect schema load table\n",
    "6. Create Dataflow to read data from bigquery and not to do any preprocessing and write into cloud storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required pre-requisite libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables\n",
    "PROJECT = 'google-cloud-project'\n",
    "BUCKET = 'google-cloud-bucket-name'\n",
    "DATASET = 'credit_card_fraud_detection'\n",
    "TABLE = 'creditcard'\n",
    "URI = 'https://storage.googleapis.com/advanced-solutions-lab/fraud/creditcard.csv'\n",
    "GS_URI = 'gs://{}/credit-card-fraud-detection/creditcard-schema.csv'.format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['DATASET'] = DATASET\n",
    "os.environ['TABLE'] = TABLE\n",
    "os.environ['URI'] = URI\n",
    "os.environ['GS_URI'] = GS_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storage_bucket(bucket_name):\n",
    "    \"\"\"Creates a new bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.create_bucket(bucket_name)\n",
    "    print('Bucket {} created'.format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_storage_bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bigquery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_dataset(dataset_name):\n",
    "  \"\"\"Creates a new bigquery dataset.\"\"\"\n",
    "  # Get Client object\n",
    "  bigquery_client = bigquery.Client()\n",
    "  # create dataset reference object\n",
    "  dataset_ref = bigquery_client.dataset(dataset_name)\n",
    "  # Construct a full Dataset object to send to the API.\n",
    "  dataset = bigquery.Dataset(dataset_ref)\n",
    "  # Specify the geographic location where the dataset should reside.\n",
    "  dataset.location = 'US'\n",
    "  # Send the dataset to the API for creation.\n",
    "  # Raises google.api_core.exceptions.AlreadyExists if the Dataset already\n",
    "  # exists within the project.\n",
    "  dataset = bigquery_client.create_dataset(dataset)\n",
    "  print('Bigquery dataset {} created'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bigquery_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download credit card fraud detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget $URI -O ./data/creditcard-download.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $PWD/data/creditcard-download.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/creditcard-download.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/creditcard-schema.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $PWD/data/creditcard-schema.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil -m rm -rf gs://${BUCKET}/credit-card-fraud-detection/*\n",
    "gsutil -m cp data/*.csv gs://${BUCKET}/credit-card-fraud-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bigquery_table_from_uri(dataset_name, table_name, uri):\n",
    "  \"\"\"Loads data into bigquery table from uri source.\"\"\"\n",
    "  # Get Client object\n",
    "  bigquery_client = bigquery.Client()\n",
    "  \n",
    "  dataset_ref = bigquery_client.dataset(dataset_name)\n",
    "  job_config = bigquery.LoadJobConfig()\n",
    "  job_config.autodetect = True\n",
    "  job_config.skip_leading_rows = 1\n",
    "  # The source format defaults to CSV, so the line below is optional.\n",
    "  job_config.source_format = bigquery.SourceFormat.CSV\n",
    "\n",
    "  load_job = bigquery_client.load_table_from_uri(\n",
    "      uri,\n",
    "      dataset_ref.table(table_name),\n",
    "      job_config=job_config)  # API request\n",
    "  print('Starting job {}'.format(load_job.job_id))\n",
    "\n",
    "  load_job.result()  # Waits for table load to complete.\n",
    "  print('Job finished.')\n",
    "\n",
    "  destination_table = bigquery_client.get_table(dataset_ref.table(table_name))\n",
    "  print('Loaded {} rows.'.format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bigquery_table_from_uri(DATASET, TABLE, GS_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "# Copyright 2018 Atos. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
